{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5j7N_VzvAA-"
   },
   "source": [
    "#### Train Robots to Shoot Lazers at Aliens With Software Funded By Elon Musk and Microsoft\n",
    "\n",
    "In this introduction I'm going to skip ahead to the robots shooting lazers because life is too short. Don't be intimidated by the code below, if you stick around after the lazers I'll explain everything that's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "fbfhPDknco7C",
    "outputId": "26b2db12-4065-4443-fbcc-0dec377f051a"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "class RandomAgentContainer:\n",
    "    \"\"\"A model that takes random actions and doesn't learn\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def decision_function(self, env):\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    def train_on_history(self, history):\n",
    "        pass\n",
    "model = RandomAgentContainer()\n",
    "\n",
    "\n",
    "\n",
    "def run_job(env_name, model, steps, episodes_per_train=1,\n",
    "            verbose=False, video_dir=False):\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Video output\n",
    "    if not video_dir:\n",
    "        video_dir = \"./gym-results\"\n",
    "    \n",
    "    env = wrappers.Monitor(env, video_dir, force=True)\n",
    "    \n",
    "    # initial observation & reset\n",
    "    observation = env.reset()\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    episode = 0\n",
    "    \n",
    "    for i in range(steps):\n",
    "        action = model.decision_function(env)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        history.append({\n",
    "            \"episode\": episode,\n",
    "            \"observation\": observation,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"info\": info,\n",
    "            \"action\": action,\n",
    "            \"step\": i\n",
    "        })\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Done with episode {episode}, {info}\")\n",
    "            \n",
    "            # Train model\n",
    "            if episodes_per_train and episode % episodes_per_train==0:\n",
    "                model.train_on_history(history)\n",
    "            \n",
    "            \n",
    "            episode += 1\n",
    "            env.reset()\n",
    "    env.close()\n",
    "    data = {\n",
    "        'history': history,\n",
    "        'env':env,\n",
    "        'parameters': {\n",
    "            'steps': steps,\n",
    "            'env_name': env_name,\n",
    "            'episodes_per_train': episodes_per_train,\n",
    "            'video_dir': video_dir\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    return data\n",
    "\n",
    "result = run_job(\"SpaceInvaders-v0\", model,\n",
    "        1000, episodes_per_train=0);\n",
    "\n",
    "def render_video(episode_num, env, video_dir=None):\n",
    "    if not video_dir:\n",
    "        video_dir = './gym-results'\n",
    "    video_file_handle = '{}/openaigym.video.{}.video{:0>6}.mp4'.format(video_dir, env.file_infix, episode_num)\n",
    "    with io.open(video_file_handle, 'r+b') as file:\n",
    "        video = file.read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        return HTML(data='''\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "    .format(encoded.decode('ascii')))\n",
    "    \n",
    "render_video(0, result['env']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TAdF3PBdItZ"
   },
   "source": [
    "And that's all it took! We wrote some code and it made the robot shoot the lazers. If that's what you came for, now's the time to leave. *But*, if you're interested in adding reinforcement learning to your toolbox, I hope you'll read on. And now we'll get to the topic behind the topic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Reinforcement Learning\n",
    "\n",
    "I'm going to devote some time to writing about beginner/intermediate RL solutions starting with a framework called OpenAI Gym. OpenAI (and Gym) have been around for years as Elon Musk and Sam Altman's AI startup, but it has [just now](https://twitter.com/satyanadella/status/1153351797223768068?s=20) come out that OpenAI will colab with Microsoft's public cloud. OpenAI is still actively maintained and adding features at time of writing. In this introduction we're going to unpack basic concepts and explore the code you saw above line-by-line to help understand OpenAI Gym.\n",
    "\n",
    "**With that out of the way....** We're going to set aside the code you saw above for a little while and introduce core concepts at...\n",
    "\n",
    "![](https://static.tvtropes.org/pmwiki/pub/images/ludicrous_speed.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Besides\n",
    "\n",
    "I've seen too many articles titled something like \"Machine Learning vs AI What's the Difference and Don't You CARE?\" Let's get through this in as few words as possible, and with some serious oversimplifiying.\n",
    "\n",
    "**Machine Learning/AI**: The machine predicts something that it hasn't seen before using math or something. Wow! -- All the other categories are subsets of this.\n",
    "\n",
    "**Supervised Learning**: We hide something (or multiple things) from the machine and ask it to predict what we hid. Because we have the hidden stuff, we know when the machine is wrong and use that information to improve the performance of the predictor by \"training\" it.\n",
    "\n",
    "**Unsupervised Learning**: We have data, but we don't have the column that we would hide from the machine in the supvised learning category. Nonetheless we ask the machine to remark on the data. Often this involves sorting the data into \"clusters.\" \n",
    "\n",
    "**Reinforcement Learning**: We tell the machine \"go drive a car lol.\" We give it a high-five and a reward if it doesn't crash (or whatever.) If it crashes we punish it somehow. It doesn't have to be a car: it could be a video game, or a robot, or an elastic compute instance on the public cloud, or a bunch of other things. The car is simulated -- maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hit the Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly unpack the code I shared earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI gym is a framework for managing the interactions, outputs, and rendering for **environments** and **agents**. We'll start by creating an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgentContainer:\n",
    "    \"\"\"A model that takes random actions and doesn't learn\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def decision_function(self, env):\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "    def train(self, o):\n",
    "        pass\n",
    "        \n",
    "    def train_on_history(self, history):\n",
    "        pass\n",
    "\n",
    "model = RandomAgentContainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"random agent\" takes a random action by sampling the `action_space`. For example, in a Nintendo game, the \"action space\" is basically just the collection of all buttons on the controller. I decided to standardize my agents to always have `train`, `train_on_history` and `decision_function` methods, but this agent never really trains since it always uses the same rule to make a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def run_job(env_name, model, steps, episodes_per_train=1,\n",
    "            verbose=False, video_dir=False)\n",
    "```\n",
    "\n",
    "This is a helper function that standardizes how we train agents. OpenAI is somewhat unopinionated about how you train agents  so stuff like this is left up to us.\n",
    "\n",
    "`env_name`: gets passed to `gym.make()` which wants a string like `SpaceInvaders-v0`.\n",
    "\n",
    "`model`: an instance of one of our agent classes.\n",
    "\n",
    "`steps`: how many times the environment will run the `step()` method (we'll explore that soon).\n",
    "\n",
    "`episodes_per_train`: how many steps between each call to `train_on_history` for the agent?\n",
    "\n",
    "`verbose`: is checked before printing anything.\n",
    "\n",
    "`video_dir`: is where the renderings are stored. Defaults to \"gym_results\"\n",
    "\n",
    "After being called, our helper function starts with:\n",
    "\n",
    "```python\n",
    "    env = gym.make(env_name)\n",
    "```\n",
    "\n",
    "You might call this the entrance to the gym: this creates the environment instance that we'll use throughout the rest of the function. The next couple lines take care of video recording settings:\n",
    "```python\n",
    "    # Video output\n",
    "    if not video_dir:\n",
    "        video_dir = \"./gym-results\"\n",
    "    \n",
    "    env = wrappers.Monitor(env, video_dir, force=True)\n",
    "```\n",
    "\n",
    "Then we do some initialization:\n",
    "```python\n",
    "    # initial observation & reset\n",
    "    observation = env.reset()\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    episode = 0\n",
    "```\n",
    "\n",
    "By convention, we `reset` the environment before taking any other action. I also want the history of our steps, but this can be cumbersome in some environments so we'll disable or parameterize that in the future. Finally, I want a count of our episodes, since each episode gets its own video and JSON file with a name derived from that number (among other things.)\n",
    "\n",
    "```python\n",
    "    for i in range(steps):\n",
    "        action = model.decision_function(env)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "```\n",
    "This is the main flow of a `step` in gym. \n",
    "\n",
    "1. The agent model makes a decision based on some information from the environment. \n",
    "2. The environment advances a \"step\" with that action.\n",
    "3. The step produces a new **observation** showing the new state of affairs in the environment that resulted from the action.\n",
    "4. The step applies a **reward function** to calculate the reward (positive or negative) that the model will get.\n",
    "5. The step returns additional information about the environment including whether the environment is **\"done\"** and any other data in **info**.\n",
    "\n",
    "In Space Invaders the `observation` is a numerical representation of a screen. In many RL use cases we're responsible for writing our own reward function to evaluate the actions of our models, but in OpenAI gym it's provided for us. The environment is \"done\" when we win or lose. Every full interaction with the environment is referred to as an \"episode\" so when we're \"done\" we move to the next episode.\n",
    "\n",
    "All this happens while a handful of frames, or even just one frame, display in the video.\n",
    "\n",
    "```python\n",
    "        history.append({\n",
    "            \"episode\": episode,\n",
    "            \"observation\": observation,\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "            \"info\": info,\n",
    "            \"action\": action,\n",
    "            \"step\": i\n",
    "        })\n",
    "```\n",
    "Next, I record a bunch of data about the step we just took. I'm skeptical that you would ever record all this, especially `observation` which is basically an image. In any case, this gives you an idea for how much information is available to you at each step.\n",
    "\n",
    "Lastly, we have some special actions to take if we're `done`:\n",
    "```python\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Done with episode {episode}, {info}\")\n",
    "            \n",
    "            # Train model\n",
    "            if episodes_per_train and episode % episodes_per_train==0:\n",
    "                model.train_on_history(history)\n",
    "            \n",
    "            \n",
    "            episode += 1\n",
    "            env.reset()\n",
    "\n",
    "```\n",
    "\n",
    "We optionally `print`, we optionally `train`, we increment the `episode` counter, and we `reset` the environment. This is very much like hitting the RESET button on an old console.\n",
    "\n",
    "```python\n",
    "    env.close()\n",
    "    data = {\n",
    "        'history': history,\n",
    "        'env':env,\n",
    "        'parameters': {\n",
    "            'steps': steps,\n",
    "            'env_name': env_name,\n",
    "            'episodes_per_train': episodes_per_train,\n",
    "            'video_dir': video_dir\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    return data\n",
    "```\n",
    "\n",
    "Once the loop is finished we close the environment and return all the data we collected. We can use that data for our video_rendering function, which is mainly just a bunch of I/O ops, parsing to a Jupyter-friendly object, and this important string manipulation:\n",
    "\n",
    "```python\n",
    "video_file_handle = '{}/openaigym.video.{}.video{:0>6}.mp4'.format(video_dir, env.file_infix, episode_num)\n",
    "```\n",
    "And this is just to conform our filepath to OpenAI's naming convention for files, ensuring we find the right one.\n",
    "\n",
    "#### In Review\n",
    "\n",
    "In this introduction you saw robots shoot lazers at aliens. What more do you want? Okay well you also:\n",
    "\n",
    "1. Learned about RL and about how it differs from other ML learning methodologies\n",
    "2. Learned about OpenAI and their Environment/Agent model\n",
    "3. Took a brief tour through the execution pattern involved for an environment's `step`.\n",
    "4. Saw the entire environment lifecycle: first `.make`, then `done`, `.reset`, and `.step` cycles, and finally `.close`.\n",
    "\n",
    "Next time maybe we'll land on the moon or something. Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "openAIExplr.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
